\section{Our Approach}

\textbf{Trying different backbones}

While using a backbone speeds up considerably the time that it takes to train a model, it is not straightforward which one will work better for a specific application. For that, we built a small experiment where we tested different pre-trained CNN models and their parameters (such as \textit{batch_size} and \textit{num_epochs}). Using these models sped up the process considerably, as we just had to train the heads of each model.

In total, we tried five different modern CNN architectures: ResNet, AlexNet, VGG, SqueezeNet and DenseNet. For each of them, we tried different values for the aforementioned parameters. In all the different settings we got a similar accuracy score, ranging from 0.90 to 0.93. These results are at the same time promising and discouraging. On the one hand, the specific pre-trained model chosen did not have a huge influence in the accuracy, so we could choose the fastest one to train. On the other hand, this prevented us from having an easy way of improving our general results, as it took away one possible part to optimize. 

In the end, we choose ResNet as our backbone, as it gave the highest accuracy score that we got in the experiment(0.9346) while using small values for \textit{batch_size} and \textit{num_epochs}. Deciding for ResNet gave the model the largest accuracy possible while being also the fastest to train. This made it perfect to get scaled later for training with the whole dataset.

{\it TODO: decribe Faster R-CNN and our modifications}
