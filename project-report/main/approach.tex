\section{Our Approach}

\textbf{Trying different backbones}

While using a backbone speeds up considerably the time that it takes to train a model, it is not straightforward which one will work better for a specific application. For that, we built a small experiment where we tested different pre-trained CNN models and their parameters (such as \textit{batch_size} and \textit{num_epochs}). Using these models sped up the process considerably, as we just had to train the heads of each model.

In total, we tried five different modern CNN architectures: ResNet, AlexNet, VGG, SqueezeNet and DenseNet. For each of them, we tried different values for the aforementioned parameters. In all the different settings we got a similar accuracy score, ranging from 0.90 to 0.93. These results are at the same time promising and discouraging. On the one hand, the specific pre-trained model chosen did not have a huge influence in the accuracy, so we could choose the fastest one to train. On the other hand, this prevented us from having a promising way of improving our general results, as it took away one possible part to optimize. 

{\it TODO: decribe Faster R-CNN and our modifications}
